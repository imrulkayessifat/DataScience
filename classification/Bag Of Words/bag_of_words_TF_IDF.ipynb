{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bag_of_words_TF-IDF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCs7NmkE00lh"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhog7L0Q06jz"
      },
      "source": [
        "data_imdb = r\"https://drive.google.com/file/d/1QtSgzb9eF0cqXBd0cFLpDlBG9LfBh5UK/view?usp=sharing\"\n",
        "data_imdb = r\"https://drive.google.com/uc?id=\" + data_imdb.split('/')[-2]\n",
        "data_imdb = pd.read_csv(data_imdb,delimiter='\\t',header=None)\n",
        "data_imdb.columns = [\"Review_text\",\"Review class\"]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-RYEuP708N9"
      },
      "source": [
        "data_amazon = r\"https://drive.google.com/file/d/1czEh8Ga2Uo2omdeMZECbQsjtKFqomq1O/view?usp=sharing\"\n",
        "data_amazon = r\"https://drive.google.com/uc?id=\" + data_amazon.split('/')[-2]\n",
        "data_amazon = pd.read_csv(data_amazon,delimiter='\\t',header=None)\n",
        "data_amazon.columns = [\"Review_text\",\"Review class\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TYm_SxC091u"
      },
      "source": [
        "data_yelp = r\"https://drive.google.com/file/d/1_ev3N05EjCy3P3vm5D_nSSG9GlTf2uDJ/view?usp=sharing\"\n",
        "data_yelp = r\"https://drive.google.com/uc?id=\" + data_yelp.split('/')[-2]\n",
        "data_yelp = pd.read_csv(data_yelp,delimiter='\\t',header=None)\n",
        "data_yelp.columns = [\"Review_text\",\"Review class\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alPSqFlA0_lo"
      },
      "source": [
        "data = pd.concat([data_imdb,data_amazon,data_yelp])\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAwdDxZt1GR_",
        "outputId": "9805ec5c-dd3d-4b8e-f9bb-21dffbcd4c3d"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvDpccB01O3R"
      },
      "source": [
        "def clean_text(df):\n",
        "    all_reviews = list()\n",
        "    lines = df[\"Review_text\"].values.tolist()\n",
        "    for text in lines:\n",
        "        text = text.lower()\n",
        "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "        text = pattern.sub('', text)\n",
        "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
        "        tokens = word_tokenize(text)\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        stripped = [w.translate(table) for w in tokens]\n",
        "        words = [word for word in stripped if word.isalpha()]\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        stop_words.discard(\"not\")\n",
        "        PS = PorterStemmer()\n",
        "        words = [PS.stem(w) for w in words if not w in stop_words]\n",
        "        words = ' '.join(words)\n",
        "        all_reviews.append(words)\n",
        "    return all_reviews\n",
        "\n",
        "all_reviews = clean_text(data)\n",
        "all_reviews[0:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deBQT5KI1SQr"
      },
      "source": [
        "stopwords.words(\"english\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpxoCy_v1Xlo"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "TV = TfidfVectorizer(min_df=3) \n",
        "X = TV.fit_transform(all_reviews).toarray()\n",
        "y = data['Review class'].values\n",
        "print(np.shape(X))\n",
        "print(np.shape(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R41RLScp1fQv"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVE9mpDS1jTZ",
        "outputId": "47ce798a-4d32-4f12-94bf-887388989faf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# model = DecisionTreeClassifier(criterion=\"entropy\", random_state=41)\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(f1_score(y_test, y_pred))\n",
        "print(precision_score(y_test, y_pred))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7054545454545454\n",
            "0.7075812274368232\n",
            "0.6758620689655173\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}